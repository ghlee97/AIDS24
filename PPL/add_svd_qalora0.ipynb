{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/raid/LLM/llama2-7b\",\n",
    "    torch_dtype = torch.float32,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "for para in fp_model.parameters():\n",
    "    para.requires_grad = False\n",
    "fp_model.config.use_cache = False\n",
    "fp_model.eval()\n",
    "sd = {k:v.cpu() for k,v in fp_model.state_dict().items()}\n",
    "del fp_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_model = AutoModelForCausalLM.from_pretrained(\n",
    "    #\"/home/leegh/qloras/qalora_svd/models/llama2-7b-qalora-fake_w2-pool_first_avg\",\n",
    "    \"/raid/lgh/aids24/EX2/ex2_llama2_7b_awq_w2_scale\",\n",
    "    torch_dtype = torch.float32,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "for para in w2_model.parameters():\n",
    "    para.requires_grad = False\n",
    "w2_model.config.use_cache = False\n",
    "w2_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd[\"model.layers.0.self_attn.q_proj.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_func_asym(w, n_bits, q_group_size):\n",
    "    # FP SCALE, INT ZERO\n",
    "\n",
    "    org_w_shape = w.shape\n",
    "    # q_group_size = -1\n",
    "    \n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "    else:\n",
    "        w = w.reshape(-1, w.shape[-1]) # channel-wise\n",
    "\n",
    "    max_val = w.amax(dim=1, keepdim=True)\n",
    "    min_val = w.amin(dim=1, keepdim=True)\n",
    "    max_int = 2 ** n_bits - 1\n",
    "    min_int = 0\n",
    "    # scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "    scales = (max_val - min_val) / max_int\n",
    "    zeros = (-torch.round(min_val / scales)).clamp_(min_int, max_int)\n",
    "    \n",
    "    w = (torch.clamp(torch.round(w / scales) +\n",
    "                    zeros, min_int, max_int) - zeros) * scales\n",
    "    \n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w_q = w.reshape(org_w_shape)\n",
    "    \n",
    "    return w_q.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groop_pool_weight(w, q_group_size):\n",
    "    org_w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "        return nn.AvgPool1d(q_group_size)(w).reshape(org_w_shape[0], org_w_shape[1] // q_group_size)\n",
    "        #return nn.MaxPool1d(q_group_size, q_group_size, 0)(w).reshape(org_w_shape[0], org_w_shape[1] // q_group_size)\n",
    "        #return nn.MaxPool1d(q_group_size, q_group_size, 0)(torch.abs(w)).reshape(org_w_shape[0], org_w_shape[1] // q_group_size)\n",
    "    else:\n",
    "        w = w.reshape(-1, w.shape[-1])\n",
    "        return nn.AvgPool1d(w.shape[-1])(w).reshape(org_w_shape[0], 1)\n",
    "        #return nn.MaxPool1d(w.shape[-1], w.shape[-1], 0)(w).reshape(org_w_shape[0], 1)\n",
    "        #return nn.MaxPool1d(w.shape[-1], w.shape[-1], 0)(torch.abs(w)).reshape(org_w_shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake Quant -> w2, fp zero\n",
    "groop_pool = False\n",
    "pool_first = True\n",
    "\n",
    "svd = False\n",
    "svd_rank = 4096\n",
    "\n",
    "q_bit = 2\n",
    "group_size = 64\n",
    "for n, m in w2_model.named_modules():\n",
    "    if n != 'lm_head' and isinstance(m, nn.Linear):\n",
    "        print(n)\n",
    "        temp_weight = copy.deepcopy(m.weight)\n",
    "        org_w_shape = temp_weight.shape\n",
    "        quant_weight = temp_weight\n",
    "        quant_weight = quant_func_asym(temp_weight, q_bit, group_size)\n",
    "        #quant_weight = quant_func_asym_new(temp_weight, q_bit, group_size)\n",
    "\n",
    "        if groop_pool or svd:\n",
    "            fp_weight = sd[n + \".weight\"]\n",
    "\n",
    "        if groop_pool and not svd:\n",
    "            if pool_first:\n",
    "                pool_fp_w = groop_pool_weight(fp_weight, group_size)\n",
    "                pool_w2_w = groop_pool_weight(quant_weight, group_size)\n",
    "\n",
    "                pool_err = pool_fp_w - pool_w2_w\n",
    "                pool_err_expand = torch.transpose(pool_err.reshape(-1).repeat(group_size, 1), 1, 0).reshape(org_w_shape[0], org_w_shape[1])\n",
    "                adapter_weight = pool_err_expand\n",
    "            else:\n",
    "                #raise NotImplementedError\n",
    "                q_err = fp_weight - quant_weight\n",
    "                q_err_pool = groop_pool_weight(q_err, group_size)\n",
    "                q_err_pool_expand = torch.transpose(q_err_pool.reshape(-1).repeat(group_size, 1), 1, 0).reshape(org_w_shape[0], org_w_shape[1])\n",
    "                adapter_weight = q_err_pool_expand\n",
    "        \n",
    "        elif not groop_pool and svd:\n",
    "            gap_weight = (fp_weight - quant_weight).detach().cpu()\n",
    "            U, S, Vh = torch.linalg.svd(gap_weight, full_matrices=False)\n",
    "            L = U @ (torch.sqrt(torch.diag(S)[:, 0:svd_rank])) # lora_B\n",
    "            R = torch.sqrt(torch.diag(S)[0:svd_rank, :]) @ Vh  # lora_A\n",
    "            adapter_weight = L @ R\n",
    "        \n",
    "        elif groop_pool and svd:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "        if groop_pool or svd:\n",
    "            m.weight.data = quant_weight + adapter_weight\n",
    "        else:\n",
    "            m.weight.data = quant_weight\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_model.model.layers[0].post_attention_layernorm.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_model.model.layers[0].self_attn.q_proj.weight[0][:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_model.model.layers[0].self_attn.q_proj.weight[0][128:192].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppl_utils import eval_ppl\n",
    "w2_model = w2_model.to(\"cuda\")\n",
    "results = eval_ppl(w2_model, False, \"llama2\", \"cuda\", \"/raid/LLM/llama2-7b\")\n",
    "w2_model = w2_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dumped = json.dumps(\n",
    "    results, indent=2, ensure_ascii=False\n",
    ")\n",
    "\n",
    "\n",
    "#output_dir = \"PPL_results/EX1/step8/5iter/svd_init_results/svd_init\"\n",
    "output_dir = \"PPL_results/qloras/llama2-7b-rtn_w2a16g64\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "with open(os.path.join(output_dir, \"results.json\"), \"w\") as f:\n",
    "    f.write(dumped)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_model.save_pretrained(\"/raid/lgh/ex1_llama2_7b_awq_w2_fake_manual_w3scale\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/raid/LLM/llama2-7b\", use_fast=True)\n",
    "tokenizer.save_pretrained(\"/raid/lgh/ex1_llama2_7b_awq_w2_fake_manual_w3scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2_model.model.layers[0].mlp.down_proj.weight)\n",
    "print(w2_model.model.layers[0].mlp.down_proj.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2_model.model.layers[0].mlp.down_proj.weight)\n",
    "print(w2_model.model.layers[0].mlp.down_proj.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_model.model.layers[0].mlp.down_proj.weight[0][11004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = copy.deepcopy(w2_model.model.layers[0].mlp.down_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_w_shape = sample.shape\n",
    "print(org_w_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_group_size = 64\n",
    "if q_group_size > 0:\n",
    "    assert org_w_shape[-1] % q_group_size == 0\n",
    "    sample = sample.reshape(-1, q_group_size)\n",
    "else:\n",
    "    sample = sample.reshape(-1, sample.shape[-1])\n",
    "print(sample)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = org_w_shape[1] / q_group_size\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/nn.html#pooling-layers \n",
    "print(nn.AvgPool1d(64)(sample))\n",
    "print(nn.AvgPool1d(64)(sample).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MaxPool1d(q_group_size, q_group_size, 0)(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MaxPool1d(q_group_size, q_group_size, 0)(sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "din = org_w_shape[0]\n",
    "print(din)\n",
    "dout = org_w_shape[1]\n",
    "print(dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_group_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_example = nn.AvgPool1d(64)(sample).reshape(org_w_shape[0], org_w_shape[1] // q_group_size)\n",
    "print(pooled_example)\n",
    "print(pooled_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pooled_example.reshape(-1))\n",
    "print(pooled_example.reshape(-1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(pooled_example.reshape(-1).repeat(q_group_size, 1), 1, 0).reshape(org_w_shape[0], org_w_shape[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(pooled_example.reshape(-1).repeat(q_group_size, 1), 1, 0).reshape(org_w_shape[0], org_w_shape[1])[0][:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(pooled_example.reshape(-1).repeat(q_group_size, 1), 1, 0).reshape(org_w_shape[0], org_w_shape[1])[0][:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"/raid/lgh/multi_lora/EX1_5iter/step9_merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"/home/leegh/lgh_n24/models/llama2-7b-omni-w2a16g64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model_path = \"/raid/LLM/llama2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 256 ###################################################\n",
    "lora_alpha = lora_r\n",
    "lora_dropout = 0.1\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "target_linear = ['gate_proj', 'k_proj', 'o_proj', 'v_proj', 'q_proj', 'up_proj', 'down_proj']\n",
    "target_t_type = 'CAUSAL_LM'\n",
    "lora_config = LoraConfig(\n",
    "    init_lora_weights = \"gaussian\",\n",
    "    r = lora_r,\n",
    "    lora_alpha = lora_alpha,\n",
    "    target_modules = target_linear,\n",
    "    lora_dropout = lora_dropout,\n",
    "    bias = \"none\",\n",
    "    task_type = target_t_type \n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.model.model.layers[0].mlp.gate_proj.lora_A.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.model.model.layers[0].mlp.gate_proj.lora_B.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"/raid/lgh/multi_lora/EX1_10iter/step1_merged/lora_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners.lora import LoraLayer\n",
    "\n",
    "rank=lora_r\n",
    "\n",
    "for n,m in model.named_modules():\n",
    "    if isinstance(m, LoraLayer):\n",
    "        print(n)\n",
    "        adj_name = n.replace('base_model.model.','') + '.weight'\n",
    "        #gap_weight = (m.base_layer.weight - sd[adj_name]).detach().cpu()\n",
    "        gap_weight = (sd[adj_name] - m.base_layer.weight).detach().cpu()\n",
    "\n",
    "        U, S, Vh = torch.linalg.svd(gap_weight, full_matrices=False)\n",
    "        L = U @ (torch.sqrt(torch.diag(S)[:, 0:rank])) # lora_B\n",
    "        R = torch.sqrt(torch.diag(S)[0:rank, :]) @ Vh  # lora_A\n",
    "        # B @ A\n",
    "        m.lora_A.default.weight.data = R\n",
    "        m.lora_B.default.weight.data = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.model.model.layers[0].mlp.gate_proj.lora_B.default.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{base_model_path}/svd_r256_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{base_model_path}/svd_init\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniquant_kernel",
   "language": "python",
   "name": "omniquant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
