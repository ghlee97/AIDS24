{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import copy\n",
    "import json\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "#from ppl_utils import eval_ppl\n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/raid/lgh/aids24/EX1/ex1_llama2_7b_omni_w2_real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_path,\n",
    "    #torch_dtype = torch.float32, #torch.float16\n",
    "    device_map = \"cuda\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_path,\n",
    "    torch_dtype = torch.float32, #torch.float16\n",
    "    device_map = \"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "#model = PeftModel.from_pretrained(model, adapter_path, is_trainable=True)\n",
    "for para in model.parameters():\n",
    "    para.requires_grad = False\n",
    "model.config.use_cache = False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(base_path+\"/model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head.weight\n",
      "model.embed_tokens.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.mlp.down_proj.g_idx\n",
      "model.layers.0.mlp.down_proj.qweight\n",
      "model.layers.0.mlp.down_proj.qzeros\n",
      "model.layers.0.mlp.down_proj.scales\n",
      "model.layers.0.mlp.gate_proj.g_idx\n",
      "model.layers.0.mlp.gate_proj.qweight\n",
      "model.layers.0.mlp.gate_proj.qzeros\n",
      "model.layers.0.mlp.gate_proj.scales\n",
      "model.layers.0.mlp.up_proj.g_idx\n",
      "model.layers.0.mlp.up_proj.qweight\n",
      "model.layers.0.mlp.up_proj.qzeros\n",
      "model.layers.0.mlp.up_proj.scales\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.0.self_attn.k_proj.g_idx\n",
      "model.layers.0.self_attn.k_proj.qweight\n",
      "model.layers.0.self_attn.k_proj.qzeros\n",
      "model.layers.0.self_attn.k_proj.scales\n",
      "model.layers.0.self_attn.o_proj.g_idx\n",
      "model.layers.0.self_attn.o_proj.qweight\n",
      "model.layers.0.self_attn.o_proj.qzeros\n",
      "model.layers.0.self_attn.o_proj.scales\n",
      "model.layers.0.self_attn.q_proj.g_idx\n",
      "model.layers.0.self_attn.q_proj.qweight\n",
      "model.layers.0.self_attn.q_proj.qzeros\n",
      "model.layers.0.self_attn.q_proj.scales\n",
      "model.layers.0.self_attn.v_proj.g_idx\n",
      "model.layers.0.self_attn.v_proj.qweight\n",
      "model.layers.0.self_attn.v_proj.qzeros\n",
      "model.layers.0.self_attn.v_proj.scales\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.mlp.down_proj.g_idx\n",
      "model.layers.1.mlp.down_proj.qweight\n",
      "model.layers.1.mlp.down_proj.qzeros\n",
      "model.layers.1.mlp.down_proj.scales\n",
      "model.layers.1.mlp.gate_proj.g_idx\n",
      "model.layers.1.mlp.gate_proj.qweight\n",
      "model.layers.1.mlp.gate_proj.qzeros\n",
      "model.layers.1.mlp.gate_proj.scales\n",
      "model.layers.1.mlp.up_proj.g_idx\n",
      "model.layers.1.mlp.up_proj.qweight\n",
      "model.layers.1.mlp.up_proj.qzeros\n",
      "model.layers.1.mlp.up_proj.scales\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.k_proj.g_idx\n",
      "model.layers.1.self_attn.k_proj.qweight\n",
      "model.layers.1.self_attn.k_proj.qzeros\n",
      "model.layers.1.self_attn.k_proj.scales\n",
      "model.layers.1.self_attn.o_proj.g_idx\n",
      "model.layers.1.self_attn.o_proj.qweight\n",
      "model.layers.1.self_attn.o_proj.qzeros\n",
      "model.layers.1.self_attn.o_proj.scales\n",
      "model.layers.1.self_attn.q_proj.g_idx\n",
      "model.layers.1.self_attn.q_proj.qweight\n",
      "model.layers.1.self_attn.q_proj.qzeros\n",
      "model.layers.1.self_attn.q_proj.scales\n",
      "model.layers.1.self_attn.v_proj.g_idx\n",
      "model.layers.1.self_attn.v_proj.qweight\n",
      "model.layers.1.self_attn.v_proj.qzeros\n",
      "model.layers.1.self_attn.v_proj.scales\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.mlp.down_proj.g_idx\n",
      "model.layers.10.mlp.down_proj.qweight\n",
      "model.layers.10.mlp.down_proj.qzeros\n",
      "model.layers.10.mlp.down_proj.scales\n",
      "model.layers.10.mlp.gate_proj.g_idx\n",
      "model.layers.10.mlp.gate_proj.qweight\n",
      "model.layers.10.mlp.gate_proj.qzeros\n",
      "model.layers.10.mlp.gate_proj.scales\n",
      "model.layers.10.mlp.up_proj.g_idx\n",
      "model.layers.10.mlp.up_proj.qweight\n",
      "model.layers.10.mlp.up_proj.qzeros\n",
      "model.layers.10.mlp.up_proj.scales\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.k_proj.g_idx\n",
      "model.layers.10.self_attn.k_proj.qweight\n",
      "model.layers.10.self_attn.k_proj.qzeros\n",
      "model.layers.10.self_attn.k_proj.scales\n",
      "model.layers.10.self_attn.o_proj.g_idx\n",
      "model.layers.10.self_attn.o_proj.qweight\n",
      "model.layers.10.self_attn.o_proj.qzeros\n",
      "model.layers.10.self_attn.o_proj.scales\n",
      "model.layers.10.self_attn.q_proj.g_idx\n",
      "model.layers.10.self_attn.q_proj.qweight\n",
      "model.layers.10.self_attn.q_proj.qzeros\n",
      "model.layers.10.self_attn.q_proj.scales\n",
      "model.layers.10.self_attn.v_proj.g_idx\n",
      "model.layers.10.self_attn.v_proj.qweight\n",
      "model.layers.10.self_attn.v_proj.qzeros\n",
      "model.layers.10.self_attn.v_proj.scales\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.mlp.down_proj.g_idx\n",
      "model.layers.11.mlp.down_proj.qweight\n",
      "model.layers.11.mlp.down_proj.qzeros\n",
      "model.layers.11.mlp.down_proj.scales\n",
      "model.layers.11.mlp.gate_proj.g_idx\n",
      "model.layers.11.mlp.gate_proj.qweight\n",
      "model.layers.11.mlp.gate_proj.qzeros\n",
      "model.layers.11.mlp.gate_proj.scales\n",
      "model.layers.11.mlp.up_proj.g_idx\n",
      "model.layers.11.mlp.up_proj.qweight\n",
      "model.layers.11.mlp.up_proj.qzeros\n",
      "model.layers.11.mlp.up_proj.scales\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.k_proj.g_idx\n",
      "model.layers.11.self_attn.k_proj.qweight\n",
      "model.layers.11.self_attn.k_proj.qzeros\n",
      "model.layers.11.self_attn.k_proj.scales\n",
      "model.layers.11.self_attn.o_proj.g_idx\n",
      "model.layers.11.self_attn.o_proj.qweight\n",
      "model.layers.11.self_attn.o_proj.qzeros\n",
      "model.layers.11.self_attn.o_proj.scales\n",
      "model.layers.11.self_attn.q_proj.g_idx\n",
      "model.layers.11.self_attn.q_proj.qweight\n",
      "model.layers.11.self_attn.q_proj.qzeros\n",
      "model.layers.11.self_attn.q_proj.scales\n",
      "model.layers.11.self_attn.v_proj.g_idx\n",
      "model.layers.11.self_attn.v_proj.qweight\n",
      "model.layers.11.self_attn.v_proj.qzeros\n",
      "model.layers.11.self_attn.v_proj.scales\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.mlp.down_proj.g_idx\n",
      "model.layers.12.mlp.down_proj.qweight\n",
      "model.layers.12.mlp.down_proj.qzeros\n",
      "model.layers.12.mlp.down_proj.scales\n",
      "model.layers.12.mlp.gate_proj.g_idx\n",
      "model.layers.12.mlp.gate_proj.qweight\n",
      "model.layers.12.mlp.gate_proj.qzeros\n",
      "model.layers.12.mlp.gate_proj.scales\n",
      "model.layers.12.mlp.up_proj.g_idx\n",
      "model.layers.12.mlp.up_proj.qweight\n",
      "model.layers.12.mlp.up_proj.qzeros\n",
      "model.layers.12.mlp.up_proj.scales\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.k_proj.g_idx\n",
      "model.layers.12.self_attn.k_proj.qweight\n",
      "model.layers.12.self_attn.k_proj.qzeros\n",
      "model.layers.12.self_attn.k_proj.scales\n",
      "model.layers.12.self_attn.o_proj.g_idx\n",
      "model.layers.12.self_attn.o_proj.qweight\n",
      "model.layers.12.self_attn.o_proj.qzeros\n",
      "model.layers.12.self_attn.o_proj.scales\n",
      "model.layers.12.self_attn.q_proj.g_idx\n",
      "model.layers.12.self_attn.q_proj.qweight\n",
      "model.layers.12.self_attn.q_proj.qzeros\n",
      "model.layers.12.self_attn.q_proj.scales\n",
      "model.layers.12.self_attn.v_proj.g_idx\n",
      "model.layers.12.self_attn.v_proj.qweight\n",
      "model.layers.12.self_attn.v_proj.qzeros\n",
      "model.layers.12.self_attn.v_proj.scales\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.mlp.down_proj.g_idx\n",
      "model.layers.13.mlp.down_proj.qweight\n",
      "model.layers.13.mlp.down_proj.qzeros\n",
      "model.layers.13.mlp.down_proj.scales\n",
      "model.layers.13.mlp.gate_proj.g_idx\n",
      "model.layers.13.mlp.gate_proj.qweight\n",
      "model.layers.13.mlp.gate_proj.qzeros\n",
      "model.layers.13.mlp.gate_proj.scales\n",
      "model.layers.13.mlp.up_proj.g_idx\n",
      "model.layers.13.mlp.up_proj.qweight\n",
      "model.layers.13.mlp.up_proj.qzeros\n",
      "model.layers.13.mlp.up_proj.scales\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.k_proj.g_idx\n",
      "model.layers.13.self_attn.k_proj.qweight\n",
      "model.layers.13.self_attn.k_proj.qzeros\n",
      "model.layers.13.self_attn.k_proj.scales\n",
      "model.layers.13.self_attn.o_proj.g_idx\n",
      "model.layers.13.self_attn.o_proj.qweight\n",
      "model.layers.13.self_attn.o_proj.qzeros\n",
      "model.layers.13.self_attn.o_proj.scales\n",
      "model.layers.13.self_attn.q_proj.g_idx\n",
      "model.layers.13.self_attn.q_proj.qweight\n",
      "model.layers.13.self_attn.q_proj.qzeros\n",
      "model.layers.13.self_attn.q_proj.scales\n",
      "model.layers.13.self_attn.v_proj.g_idx\n",
      "model.layers.13.self_attn.v_proj.qweight\n",
      "model.layers.13.self_attn.v_proj.qzeros\n",
      "model.layers.13.self_attn.v_proj.scales\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.mlp.down_proj.g_idx\n",
      "model.layers.14.mlp.down_proj.qweight\n",
      "model.layers.14.mlp.down_proj.qzeros\n",
      "model.layers.14.mlp.down_proj.scales\n",
      "model.layers.14.mlp.gate_proj.g_idx\n",
      "model.layers.14.mlp.gate_proj.qweight\n",
      "model.layers.14.mlp.gate_proj.qzeros\n",
      "model.layers.14.mlp.gate_proj.scales\n",
      "model.layers.14.mlp.up_proj.g_idx\n",
      "model.layers.14.mlp.up_proj.qweight\n",
      "model.layers.14.mlp.up_proj.qzeros\n",
      "model.layers.14.mlp.up_proj.scales\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.k_proj.g_idx\n",
      "model.layers.14.self_attn.k_proj.qweight\n",
      "model.layers.14.self_attn.k_proj.qzeros\n",
      "model.layers.14.self_attn.k_proj.scales\n",
      "model.layers.14.self_attn.o_proj.g_idx\n",
      "model.layers.14.self_attn.o_proj.qweight\n",
      "model.layers.14.self_attn.o_proj.qzeros\n",
      "model.layers.14.self_attn.o_proj.scales\n",
      "model.layers.14.self_attn.q_proj.g_idx\n",
      "model.layers.14.self_attn.q_proj.qweight\n",
      "model.layers.14.self_attn.q_proj.qzeros\n",
      "model.layers.14.self_attn.q_proj.scales\n",
      "model.layers.14.self_attn.v_proj.g_idx\n",
      "model.layers.14.self_attn.v_proj.qweight\n",
      "model.layers.14.self_attn.v_proj.qzeros\n",
      "model.layers.14.self_attn.v_proj.scales\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.mlp.down_proj.g_idx\n",
      "model.layers.15.mlp.down_proj.qweight\n",
      "model.layers.15.mlp.down_proj.qzeros\n",
      "model.layers.15.mlp.down_proj.scales\n",
      "model.layers.15.mlp.gate_proj.g_idx\n",
      "model.layers.15.mlp.gate_proj.qweight\n",
      "model.layers.15.mlp.gate_proj.qzeros\n",
      "model.layers.15.mlp.gate_proj.scales\n",
      "model.layers.15.mlp.up_proj.g_idx\n",
      "model.layers.15.mlp.up_proj.qweight\n",
      "model.layers.15.mlp.up_proj.qzeros\n",
      "model.layers.15.mlp.up_proj.scales\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.k_proj.g_idx\n",
      "model.layers.15.self_attn.k_proj.qweight\n",
      "model.layers.15.self_attn.k_proj.qzeros\n",
      "model.layers.15.self_attn.k_proj.scales\n",
      "model.layers.15.self_attn.o_proj.g_idx\n",
      "model.layers.15.self_attn.o_proj.qweight\n",
      "model.layers.15.self_attn.o_proj.qzeros\n",
      "model.layers.15.self_attn.o_proj.scales\n",
      "model.layers.15.self_attn.q_proj.g_idx\n",
      "model.layers.15.self_attn.q_proj.qweight\n",
      "model.layers.15.self_attn.q_proj.qzeros\n",
      "model.layers.15.self_attn.q_proj.scales\n",
      "model.layers.15.self_attn.v_proj.g_idx\n",
      "model.layers.15.self_attn.v_proj.qweight\n",
      "model.layers.15.self_attn.v_proj.qzeros\n",
      "model.layers.15.self_attn.v_proj.scales\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.mlp.down_proj.g_idx\n",
      "model.layers.16.mlp.down_proj.qweight\n",
      "model.layers.16.mlp.down_proj.qzeros\n",
      "model.layers.16.mlp.down_proj.scales\n",
      "model.layers.16.mlp.gate_proj.g_idx\n",
      "model.layers.16.mlp.gate_proj.qweight\n",
      "model.layers.16.mlp.gate_proj.qzeros\n",
      "model.layers.16.mlp.gate_proj.scales\n",
      "model.layers.16.mlp.up_proj.g_idx\n",
      "model.layers.16.mlp.up_proj.qweight\n",
      "model.layers.16.mlp.up_proj.qzeros\n",
      "model.layers.16.mlp.up_proj.scales\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.k_proj.g_idx\n",
      "model.layers.16.self_attn.k_proj.qweight\n",
      "model.layers.16.self_attn.k_proj.qzeros\n",
      "model.layers.16.self_attn.k_proj.scales\n",
      "model.layers.16.self_attn.o_proj.g_idx\n",
      "model.layers.16.self_attn.o_proj.qweight\n",
      "model.layers.16.self_attn.o_proj.qzeros\n",
      "model.layers.16.self_attn.o_proj.scales\n",
      "model.layers.16.self_attn.q_proj.g_idx\n",
      "model.layers.16.self_attn.q_proj.qweight\n",
      "model.layers.16.self_attn.q_proj.qzeros\n",
      "model.layers.16.self_attn.q_proj.scales\n",
      "model.layers.16.self_attn.v_proj.g_idx\n",
      "model.layers.16.self_attn.v_proj.qweight\n",
      "model.layers.16.self_attn.v_proj.qzeros\n",
      "model.layers.16.self_attn.v_proj.scales\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.mlp.down_proj.g_idx\n",
      "model.layers.17.mlp.down_proj.qweight\n",
      "model.layers.17.mlp.down_proj.qzeros\n",
      "model.layers.17.mlp.down_proj.scales\n",
      "model.layers.17.mlp.gate_proj.g_idx\n",
      "model.layers.17.mlp.gate_proj.qweight\n",
      "model.layers.17.mlp.gate_proj.qzeros\n",
      "model.layers.17.mlp.gate_proj.scales\n",
      "model.layers.17.mlp.up_proj.g_idx\n",
      "model.layers.17.mlp.up_proj.qweight\n",
      "model.layers.17.mlp.up_proj.qzeros\n",
      "model.layers.17.mlp.up_proj.scales\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.k_proj.g_idx\n",
      "model.layers.17.self_attn.k_proj.qweight\n",
      "model.layers.17.self_attn.k_proj.qzeros\n",
      "model.layers.17.self_attn.k_proj.scales\n",
      "model.layers.17.self_attn.o_proj.g_idx\n",
      "model.layers.17.self_attn.o_proj.qweight\n",
      "model.layers.17.self_attn.o_proj.qzeros\n",
      "model.layers.17.self_attn.o_proj.scales\n",
      "model.layers.17.self_attn.q_proj.g_idx\n",
      "model.layers.17.self_attn.q_proj.qweight\n",
      "model.layers.17.self_attn.q_proj.qzeros\n",
      "model.layers.17.self_attn.q_proj.scales\n",
      "model.layers.17.self_attn.v_proj.g_idx\n",
      "model.layers.17.self_attn.v_proj.qweight\n",
      "model.layers.17.self_attn.v_proj.qzeros\n",
      "model.layers.17.self_attn.v_proj.scales\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.mlp.down_proj.g_idx\n",
      "model.layers.18.mlp.down_proj.qweight\n",
      "model.layers.18.mlp.down_proj.qzeros\n",
      "model.layers.18.mlp.down_proj.scales\n",
      "model.layers.18.mlp.gate_proj.g_idx\n",
      "model.layers.18.mlp.gate_proj.qweight\n",
      "model.layers.18.mlp.gate_proj.qzeros\n",
      "model.layers.18.mlp.gate_proj.scales\n",
      "model.layers.18.mlp.up_proj.g_idx\n",
      "model.layers.18.mlp.up_proj.qweight\n",
      "model.layers.18.mlp.up_proj.qzeros\n",
      "model.layers.18.mlp.up_proj.scales\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.k_proj.g_idx\n",
      "model.layers.18.self_attn.k_proj.qweight\n",
      "model.layers.18.self_attn.k_proj.qzeros\n",
      "model.layers.18.self_attn.k_proj.scales\n",
      "model.layers.18.self_attn.o_proj.g_idx\n",
      "model.layers.18.self_attn.o_proj.qweight\n",
      "model.layers.18.self_attn.o_proj.qzeros\n",
      "model.layers.18.self_attn.o_proj.scales\n",
      "model.layers.18.self_attn.q_proj.g_idx\n",
      "model.layers.18.self_attn.q_proj.qweight\n",
      "model.layers.18.self_attn.q_proj.qzeros\n",
      "model.layers.18.self_attn.q_proj.scales\n",
      "model.layers.18.self_attn.v_proj.g_idx\n",
      "model.layers.18.self_attn.v_proj.qweight\n",
      "model.layers.18.self_attn.v_proj.qzeros\n",
      "model.layers.18.self_attn.v_proj.scales\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.mlp.down_proj.g_idx\n",
      "model.layers.19.mlp.down_proj.qweight\n",
      "model.layers.19.mlp.down_proj.qzeros\n",
      "model.layers.19.mlp.down_proj.scales\n",
      "model.layers.19.mlp.gate_proj.g_idx\n",
      "model.layers.19.mlp.gate_proj.qweight\n",
      "model.layers.19.mlp.gate_proj.qzeros\n",
      "model.layers.19.mlp.gate_proj.scales\n",
      "model.layers.19.mlp.up_proj.g_idx\n",
      "model.layers.19.mlp.up_proj.qweight\n",
      "model.layers.19.mlp.up_proj.qzeros\n",
      "model.layers.19.mlp.up_proj.scales\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.k_proj.g_idx\n",
      "model.layers.19.self_attn.k_proj.qweight\n",
      "model.layers.19.self_attn.k_proj.qzeros\n",
      "model.layers.19.self_attn.k_proj.scales\n",
      "model.layers.19.self_attn.o_proj.g_idx\n",
      "model.layers.19.self_attn.o_proj.qweight\n",
      "model.layers.19.self_attn.o_proj.qzeros\n",
      "model.layers.19.self_attn.o_proj.scales\n",
      "model.layers.19.self_attn.q_proj.g_idx\n",
      "model.layers.19.self_attn.q_proj.qweight\n",
      "model.layers.19.self_attn.q_proj.qzeros\n",
      "model.layers.19.self_attn.q_proj.scales\n",
      "model.layers.19.self_attn.v_proj.g_idx\n",
      "model.layers.19.self_attn.v_proj.qweight\n",
      "model.layers.19.self_attn.v_proj.qzeros\n",
      "model.layers.19.self_attn.v_proj.scales\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.mlp.down_proj.g_idx\n",
      "model.layers.2.mlp.down_proj.qweight\n",
      "model.layers.2.mlp.down_proj.qzeros\n",
      "model.layers.2.mlp.down_proj.scales\n",
      "model.layers.2.mlp.gate_proj.g_idx\n",
      "model.layers.2.mlp.gate_proj.qweight\n",
      "model.layers.2.mlp.gate_proj.qzeros\n",
      "model.layers.2.mlp.gate_proj.scales\n",
      "model.layers.2.mlp.up_proj.g_idx\n",
      "model.layers.2.mlp.up_proj.qweight\n",
      "model.layers.2.mlp.up_proj.qzeros\n",
      "model.layers.2.mlp.up_proj.scales\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.k_proj.g_idx\n",
      "model.layers.2.self_attn.k_proj.qweight\n",
      "model.layers.2.self_attn.k_proj.qzeros\n",
      "model.layers.2.self_attn.k_proj.scales\n",
      "model.layers.2.self_attn.o_proj.g_idx\n",
      "model.layers.2.self_attn.o_proj.qweight\n",
      "model.layers.2.self_attn.o_proj.qzeros\n",
      "model.layers.2.self_attn.o_proj.scales\n",
      "model.layers.2.self_attn.q_proj.g_idx\n",
      "model.layers.2.self_attn.q_proj.qweight\n",
      "model.layers.2.self_attn.q_proj.qzeros\n",
      "model.layers.2.self_attn.q_proj.scales\n",
      "model.layers.2.self_attn.v_proj.g_idx\n",
      "model.layers.2.self_attn.v_proj.qweight\n",
      "model.layers.2.self_attn.v_proj.qzeros\n",
      "model.layers.2.self_attn.v_proj.scales\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.mlp.down_proj.g_idx\n",
      "model.layers.20.mlp.down_proj.qweight\n",
      "model.layers.20.mlp.down_proj.qzeros\n",
      "model.layers.20.mlp.down_proj.scales\n",
      "model.layers.20.mlp.gate_proj.g_idx\n",
      "model.layers.20.mlp.gate_proj.qweight\n",
      "model.layers.20.mlp.gate_proj.qzeros\n",
      "model.layers.20.mlp.gate_proj.scales\n",
      "model.layers.20.mlp.up_proj.g_idx\n",
      "model.layers.20.mlp.up_proj.qweight\n",
      "model.layers.20.mlp.up_proj.qzeros\n",
      "model.layers.20.mlp.up_proj.scales\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.k_proj.g_idx\n",
      "model.layers.20.self_attn.k_proj.qweight\n",
      "model.layers.20.self_attn.k_proj.qzeros\n",
      "model.layers.20.self_attn.k_proj.scales\n",
      "model.layers.20.self_attn.o_proj.g_idx\n",
      "model.layers.20.self_attn.o_proj.qweight\n",
      "model.layers.20.self_attn.o_proj.qzeros\n",
      "model.layers.20.self_attn.o_proj.scales\n",
      "model.layers.20.self_attn.q_proj.g_idx\n",
      "model.layers.20.self_attn.q_proj.qweight\n",
      "model.layers.20.self_attn.q_proj.qzeros\n",
      "model.layers.20.self_attn.q_proj.scales\n",
      "model.layers.20.self_attn.v_proj.g_idx\n",
      "model.layers.20.self_attn.v_proj.qweight\n",
      "model.layers.20.self_attn.v_proj.qzeros\n",
      "model.layers.20.self_attn.v_proj.scales\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.mlp.down_proj.g_idx\n",
      "model.layers.21.mlp.down_proj.qweight\n",
      "model.layers.21.mlp.down_proj.qzeros\n",
      "model.layers.21.mlp.down_proj.scales\n",
      "model.layers.21.mlp.gate_proj.g_idx\n",
      "model.layers.21.mlp.gate_proj.qweight\n",
      "model.layers.21.mlp.gate_proj.qzeros\n",
      "model.layers.21.mlp.gate_proj.scales\n",
      "model.layers.21.mlp.up_proj.g_idx\n",
      "model.layers.21.mlp.up_proj.qweight\n",
      "model.layers.21.mlp.up_proj.qzeros\n",
      "model.layers.21.mlp.up_proj.scales\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.k_proj.g_idx\n",
      "model.layers.21.self_attn.k_proj.qweight\n",
      "model.layers.21.self_attn.k_proj.qzeros\n",
      "model.layers.21.self_attn.k_proj.scales\n",
      "model.layers.21.self_attn.o_proj.g_idx\n",
      "model.layers.21.self_attn.o_proj.qweight\n",
      "model.layers.21.self_attn.o_proj.qzeros\n",
      "model.layers.21.self_attn.o_proj.scales\n",
      "model.layers.21.self_attn.q_proj.g_idx\n",
      "model.layers.21.self_attn.q_proj.qweight\n",
      "model.layers.21.self_attn.q_proj.qzeros\n",
      "model.layers.21.self_attn.q_proj.scales\n",
      "model.layers.21.self_attn.v_proj.g_idx\n",
      "model.layers.21.self_attn.v_proj.qweight\n",
      "model.layers.21.self_attn.v_proj.qzeros\n",
      "model.layers.21.self_attn.v_proj.scales\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.mlp.down_proj.g_idx\n",
      "model.layers.22.mlp.down_proj.qweight\n",
      "model.layers.22.mlp.down_proj.qzeros\n",
      "model.layers.22.mlp.down_proj.scales\n",
      "model.layers.22.mlp.gate_proj.g_idx\n",
      "model.layers.22.mlp.gate_proj.qweight\n",
      "model.layers.22.mlp.gate_proj.qzeros\n",
      "model.layers.22.mlp.gate_proj.scales\n",
      "model.layers.22.mlp.up_proj.g_idx\n",
      "model.layers.22.mlp.up_proj.qweight\n",
      "model.layers.22.mlp.up_proj.qzeros\n",
      "model.layers.22.mlp.up_proj.scales\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.k_proj.g_idx\n",
      "model.layers.22.self_attn.k_proj.qweight\n",
      "model.layers.22.self_attn.k_proj.qzeros\n",
      "model.layers.22.self_attn.k_proj.scales\n",
      "model.layers.22.self_attn.o_proj.g_idx\n",
      "model.layers.22.self_attn.o_proj.qweight\n",
      "model.layers.22.self_attn.o_proj.qzeros\n",
      "model.layers.22.self_attn.o_proj.scales\n",
      "model.layers.22.self_attn.q_proj.g_idx\n",
      "model.layers.22.self_attn.q_proj.qweight\n",
      "model.layers.22.self_attn.q_proj.qzeros\n",
      "model.layers.22.self_attn.q_proj.scales\n",
      "model.layers.22.self_attn.v_proj.g_idx\n",
      "model.layers.22.self_attn.v_proj.qweight\n",
      "model.layers.22.self_attn.v_proj.qzeros\n",
      "model.layers.22.self_attn.v_proj.scales\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.mlp.down_proj.g_idx\n",
      "model.layers.23.mlp.down_proj.qweight\n",
      "model.layers.23.mlp.down_proj.qzeros\n",
      "model.layers.23.mlp.down_proj.scales\n",
      "model.layers.23.mlp.gate_proj.g_idx\n",
      "model.layers.23.mlp.gate_proj.qweight\n",
      "model.layers.23.mlp.gate_proj.qzeros\n",
      "model.layers.23.mlp.gate_proj.scales\n",
      "model.layers.23.mlp.up_proj.g_idx\n",
      "model.layers.23.mlp.up_proj.qweight\n",
      "model.layers.23.mlp.up_proj.qzeros\n",
      "model.layers.23.mlp.up_proj.scales\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.k_proj.g_idx\n",
      "model.layers.23.self_attn.k_proj.qweight\n",
      "model.layers.23.self_attn.k_proj.qzeros\n",
      "model.layers.23.self_attn.k_proj.scales\n",
      "model.layers.23.self_attn.o_proj.g_idx\n",
      "model.layers.23.self_attn.o_proj.qweight\n",
      "model.layers.23.self_attn.o_proj.qzeros\n",
      "model.layers.23.self_attn.o_proj.scales\n",
      "model.layers.23.self_attn.q_proj.g_idx\n",
      "model.layers.23.self_attn.q_proj.qweight\n",
      "model.layers.23.self_attn.q_proj.qzeros\n",
      "model.layers.23.self_attn.q_proj.scales\n",
      "model.layers.23.self_attn.v_proj.g_idx\n",
      "model.layers.23.self_attn.v_proj.qweight\n",
      "model.layers.23.self_attn.v_proj.qzeros\n",
      "model.layers.23.self_attn.v_proj.scales\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.mlp.down_proj.g_idx\n",
      "model.layers.24.mlp.down_proj.qweight\n",
      "model.layers.24.mlp.down_proj.qzeros\n",
      "model.layers.24.mlp.down_proj.scales\n",
      "model.layers.24.mlp.gate_proj.g_idx\n",
      "model.layers.24.mlp.gate_proj.qweight\n",
      "model.layers.24.mlp.gate_proj.qzeros\n",
      "model.layers.24.mlp.gate_proj.scales\n",
      "model.layers.24.mlp.up_proj.g_idx\n",
      "model.layers.24.mlp.up_proj.qweight\n",
      "model.layers.24.mlp.up_proj.qzeros\n",
      "model.layers.24.mlp.up_proj.scales\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.k_proj.g_idx\n",
      "model.layers.24.self_attn.k_proj.qweight\n",
      "model.layers.24.self_attn.k_proj.qzeros\n",
      "model.layers.24.self_attn.k_proj.scales\n",
      "model.layers.24.self_attn.o_proj.g_idx\n",
      "model.layers.24.self_attn.o_proj.qweight\n",
      "model.layers.24.self_attn.o_proj.qzeros\n",
      "model.layers.24.self_attn.o_proj.scales\n",
      "model.layers.24.self_attn.q_proj.g_idx\n",
      "model.layers.24.self_attn.q_proj.qweight\n",
      "model.layers.24.self_attn.q_proj.qzeros\n",
      "model.layers.24.self_attn.q_proj.scales\n",
      "model.layers.24.self_attn.v_proj.g_idx\n",
      "model.layers.24.self_attn.v_proj.qweight\n",
      "model.layers.24.self_attn.v_proj.qzeros\n",
      "model.layers.24.self_attn.v_proj.scales\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.mlp.down_proj.g_idx\n",
      "model.layers.25.mlp.down_proj.qweight\n",
      "model.layers.25.mlp.down_proj.qzeros\n",
      "model.layers.25.mlp.down_proj.scales\n",
      "model.layers.25.mlp.gate_proj.g_idx\n",
      "model.layers.25.mlp.gate_proj.qweight\n",
      "model.layers.25.mlp.gate_proj.qzeros\n",
      "model.layers.25.mlp.gate_proj.scales\n",
      "model.layers.25.mlp.up_proj.g_idx\n",
      "model.layers.25.mlp.up_proj.qweight\n",
      "model.layers.25.mlp.up_proj.qzeros\n",
      "model.layers.25.mlp.up_proj.scales\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.k_proj.g_idx\n",
      "model.layers.25.self_attn.k_proj.qweight\n",
      "model.layers.25.self_attn.k_proj.qzeros\n",
      "model.layers.25.self_attn.k_proj.scales\n",
      "model.layers.25.self_attn.o_proj.g_idx\n",
      "model.layers.25.self_attn.o_proj.qweight\n",
      "model.layers.25.self_attn.o_proj.qzeros\n",
      "model.layers.25.self_attn.o_proj.scales\n",
      "model.layers.25.self_attn.q_proj.g_idx\n",
      "model.layers.25.self_attn.q_proj.qweight\n",
      "model.layers.25.self_attn.q_proj.qzeros\n",
      "model.layers.25.self_attn.q_proj.scales\n",
      "model.layers.25.self_attn.v_proj.g_idx\n",
      "model.layers.25.self_attn.v_proj.qweight\n",
      "model.layers.25.self_attn.v_proj.qzeros\n",
      "model.layers.25.self_attn.v_proj.scales\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.mlp.down_proj.g_idx\n",
      "model.layers.26.mlp.down_proj.qweight\n",
      "model.layers.26.mlp.down_proj.qzeros\n",
      "model.layers.26.mlp.down_proj.scales\n",
      "model.layers.26.mlp.gate_proj.g_idx\n",
      "model.layers.26.mlp.gate_proj.qweight\n",
      "model.layers.26.mlp.gate_proj.qzeros\n",
      "model.layers.26.mlp.gate_proj.scales\n",
      "model.layers.26.mlp.up_proj.g_idx\n",
      "model.layers.26.mlp.up_proj.qweight\n",
      "model.layers.26.mlp.up_proj.qzeros\n",
      "model.layers.26.mlp.up_proj.scales\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.k_proj.g_idx\n",
      "model.layers.26.self_attn.k_proj.qweight\n",
      "model.layers.26.self_attn.k_proj.qzeros\n",
      "model.layers.26.self_attn.k_proj.scales\n",
      "model.layers.26.self_attn.o_proj.g_idx\n",
      "model.layers.26.self_attn.o_proj.qweight\n",
      "model.layers.26.self_attn.o_proj.qzeros\n",
      "model.layers.26.self_attn.o_proj.scales\n",
      "model.layers.26.self_attn.q_proj.g_idx\n",
      "model.layers.26.self_attn.q_proj.qweight\n",
      "model.layers.26.self_attn.q_proj.qzeros\n",
      "model.layers.26.self_attn.q_proj.scales\n",
      "model.layers.26.self_attn.v_proj.g_idx\n",
      "model.layers.26.self_attn.v_proj.qweight\n",
      "model.layers.26.self_attn.v_proj.qzeros\n",
      "model.layers.26.self_attn.v_proj.scales\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.mlp.down_proj.g_idx\n",
      "model.layers.27.mlp.down_proj.qweight\n",
      "model.layers.27.mlp.down_proj.qzeros\n",
      "model.layers.27.mlp.down_proj.scales\n",
      "model.layers.27.mlp.gate_proj.g_idx\n",
      "model.layers.27.mlp.gate_proj.qweight\n",
      "model.layers.27.mlp.gate_proj.qzeros\n",
      "model.layers.27.mlp.gate_proj.scales\n",
      "model.layers.27.mlp.up_proj.g_idx\n",
      "model.layers.27.mlp.up_proj.qweight\n",
      "model.layers.27.mlp.up_proj.qzeros\n",
      "model.layers.27.mlp.up_proj.scales\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.k_proj.g_idx\n",
      "model.layers.27.self_attn.k_proj.qweight\n",
      "model.layers.27.self_attn.k_proj.qzeros\n",
      "model.layers.27.self_attn.k_proj.scales\n",
      "model.layers.27.self_attn.o_proj.g_idx\n",
      "model.layers.27.self_attn.o_proj.qweight\n",
      "model.layers.27.self_attn.o_proj.qzeros\n",
      "model.layers.27.self_attn.o_proj.scales\n",
      "model.layers.27.self_attn.q_proj.g_idx\n",
      "model.layers.27.self_attn.q_proj.qweight\n",
      "model.layers.27.self_attn.q_proj.qzeros\n",
      "model.layers.27.self_attn.q_proj.scales\n",
      "model.layers.27.self_attn.v_proj.g_idx\n",
      "model.layers.27.self_attn.v_proj.qweight\n",
      "model.layers.27.self_attn.v_proj.qzeros\n",
      "model.layers.27.self_attn.v_proj.scales\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.mlp.down_proj.g_idx\n",
      "model.layers.28.mlp.down_proj.qweight\n",
      "model.layers.28.mlp.down_proj.qzeros\n",
      "model.layers.28.mlp.down_proj.scales\n",
      "model.layers.28.mlp.gate_proj.g_idx\n",
      "model.layers.28.mlp.gate_proj.qweight\n",
      "model.layers.28.mlp.gate_proj.qzeros\n",
      "model.layers.28.mlp.gate_proj.scales\n",
      "model.layers.28.mlp.up_proj.g_idx\n",
      "model.layers.28.mlp.up_proj.qweight\n",
      "model.layers.28.mlp.up_proj.qzeros\n",
      "model.layers.28.mlp.up_proj.scales\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.k_proj.g_idx\n",
      "model.layers.28.self_attn.k_proj.qweight\n",
      "model.layers.28.self_attn.k_proj.qzeros\n",
      "model.layers.28.self_attn.k_proj.scales\n",
      "model.layers.28.self_attn.o_proj.g_idx\n",
      "model.layers.28.self_attn.o_proj.qweight\n",
      "model.layers.28.self_attn.o_proj.qzeros\n",
      "model.layers.28.self_attn.o_proj.scales\n",
      "model.layers.28.self_attn.q_proj.g_idx\n",
      "model.layers.28.self_attn.q_proj.qweight\n",
      "model.layers.28.self_attn.q_proj.qzeros\n",
      "model.layers.28.self_attn.q_proj.scales\n",
      "model.layers.28.self_attn.v_proj.g_idx\n",
      "model.layers.28.self_attn.v_proj.qweight\n",
      "model.layers.28.self_attn.v_proj.qzeros\n",
      "model.layers.28.self_attn.v_proj.scales\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.mlp.down_proj.g_idx\n",
      "model.layers.29.mlp.down_proj.qweight\n",
      "model.layers.29.mlp.down_proj.qzeros\n",
      "model.layers.29.mlp.down_proj.scales\n",
      "model.layers.29.mlp.gate_proj.g_idx\n",
      "model.layers.29.mlp.gate_proj.qweight\n",
      "model.layers.29.mlp.gate_proj.qzeros\n",
      "model.layers.29.mlp.gate_proj.scales\n",
      "model.layers.29.mlp.up_proj.g_idx\n",
      "model.layers.29.mlp.up_proj.qweight\n",
      "model.layers.29.mlp.up_proj.qzeros\n",
      "model.layers.29.mlp.up_proj.scales\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.k_proj.g_idx\n",
      "model.layers.29.self_attn.k_proj.qweight\n",
      "model.layers.29.self_attn.k_proj.qzeros\n",
      "model.layers.29.self_attn.k_proj.scales\n",
      "model.layers.29.self_attn.o_proj.g_idx\n",
      "model.layers.29.self_attn.o_proj.qweight\n",
      "model.layers.29.self_attn.o_proj.qzeros\n",
      "model.layers.29.self_attn.o_proj.scales\n",
      "model.layers.29.self_attn.q_proj.g_idx\n",
      "model.layers.29.self_attn.q_proj.qweight\n",
      "model.layers.29.self_attn.q_proj.qzeros\n",
      "model.layers.29.self_attn.q_proj.scales\n",
      "model.layers.29.self_attn.v_proj.g_idx\n",
      "model.layers.29.self_attn.v_proj.qweight\n",
      "model.layers.29.self_attn.v_proj.qzeros\n",
      "model.layers.29.self_attn.v_proj.scales\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.mlp.down_proj.g_idx\n",
      "model.layers.3.mlp.down_proj.qweight\n",
      "model.layers.3.mlp.down_proj.qzeros\n",
      "model.layers.3.mlp.down_proj.scales\n",
      "model.layers.3.mlp.gate_proj.g_idx\n",
      "model.layers.3.mlp.gate_proj.qweight\n",
      "model.layers.3.mlp.gate_proj.qzeros\n",
      "model.layers.3.mlp.gate_proj.scales\n",
      "model.layers.3.mlp.up_proj.g_idx\n",
      "model.layers.3.mlp.up_proj.qweight\n",
      "model.layers.3.mlp.up_proj.qzeros\n",
      "model.layers.3.mlp.up_proj.scales\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.k_proj.g_idx\n",
      "model.layers.3.self_attn.k_proj.qweight\n",
      "model.layers.3.self_attn.k_proj.qzeros\n",
      "model.layers.3.self_attn.k_proj.scales\n",
      "model.layers.3.self_attn.o_proj.g_idx\n",
      "model.layers.3.self_attn.o_proj.qweight\n",
      "model.layers.3.self_attn.o_proj.qzeros\n",
      "model.layers.3.self_attn.o_proj.scales\n",
      "model.layers.3.self_attn.q_proj.g_idx\n",
      "model.layers.3.self_attn.q_proj.qweight\n",
      "model.layers.3.self_attn.q_proj.qzeros\n",
      "model.layers.3.self_attn.q_proj.scales\n",
      "model.layers.3.self_attn.v_proj.g_idx\n",
      "model.layers.3.self_attn.v_proj.qweight\n",
      "model.layers.3.self_attn.v_proj.qzeros\n",
      "model.layers.3.self_attn.v_proj.scales\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.mlp.down_proj.g_idx\n",
      "model.layers.30.mlp.down_proj.qweight\n",
      "model.layers.30.mlp.down_proj.qzeros\n",
      "model.layers.30.mlp.down_proj.scales\n",
      "model.layers.30.mlp.gate_proj.g_idx\n",
      "model.layers.30.mlp.gate_proj.qweight\n",
      "model.layers.30.mlp.gate_proj.qzeros\n",
      "model.layers.30.mlp.gate_proj.scales\n",
      "model.layers.30.mlp.up_proj.g_idx\n",
      "model.layers.30.mlp.up_proj.qweight\n",
      "model.layers.30.mlp.up_proj.qzeros\n",
      "model.layers.30.mlp.up_proj.scales\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.k_proj.g_idx\n",
      "model.layers.30.self_attn.k_proj.qweight\n",
      "model.layers.30.self_attn.k_proj.qzeros\n",
      "model.layers.30.self_attn.k_proj.scales\n",
      "model.layers.30.self_attn.o_proj.g_idx\n",
      "model.layers.30.self_attn.o_proj.qweight\n",
      "model.layers.30.self_attn.o_proj.qzeros\n",
      "model.layers.30.self_attn.o_proj.scales\n",
      "model.layers.30.self_attn.q_proj.g_idx\n",
      "model.layers.30.self_attn.q_proj.qweight\n",
      "model.layers.30.self_attn.q_proj.qzeros\n",
      "model.layers.30.self_attn.q_proj.scales\n",
      "model.layers.30.self_attn.v_proj.g_idx\n",
      "model.layers.30.self_attn.v_proj.qweight\n",
      "model.layers.30.self_attn.v_proj.qzeros\n",
      "model.layers.30.self_attn.v_proj.scales\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.mlp.down_proj.g_idx\n",
      "model.layers.31.mlp.down_proj.qweight\n",
      "model.layers.31.mlp.down_proj.qzeros\n",
      "model.layers.31.mlp.down_proj.scales\n",
      "model.layers.31.mlp.gate_proj.g_idx\n",
      "model.layers.31.mlp.gate_proj.qweight\n",
      "model.layers.31.mlp.gate_proj.qzeros\n",
      "model.layers.31.mlp.gate_proj.scales\n",
      "model.layers.31.mlp.up_proj.g_idx\n",
      "model.layers.31.mlp.up_proj.qweight\n",
      "model.layers.31.mlp.up_proj.qzeros\n",
      "model.layers.31.mlp.up_proj.scales\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.k_proj.g_idx\n",
      "model.layers.31.self_attn.k_proj.qweight\n",
      "model.layers.31.self_attn.k_proj.qzeros\n",
      "model.layers.31.self_attn.k_proj.scales\n",
      "model.layers.31.self_attn.o_proj.g_idx\n",
      "model.layers.31.self_attn.o_proj.qweight\n",
      "model.layers.31.self_attn.o_proj.qzeros\n",
      "model.layers.31.self_attn.o_proj.scales\n",
      "model.layers.31.self_attn.q_proj.g_idx\n",
      "model.layers.31.self_attn.q_proj.qweight\n",
      "model.layers.31.self_attn.q_proj.qzeros\n",
      "model.layers.31.self_attn.q_proj.scales\n",
      "model.layers.31.self_attn.v_proj.g_idx\n",
      "model.layers.31.self_attn.v_proj.qweight\n",
      "model.layers.31.self_attn.v_proj.qzeros\n",
      "model.layers.31.self_attn.v_proj.scales\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.mlp.down_proj.g_idx\n",
      "model.layers.4.mlp.down_proj.qweight\n",
      "model.layers.4.mlp.down_proj.qzeros\n",
      "model.layers.4.mlp.down_proj.scales\n",
      "model.layers.4.mlp.gate_proj.g_idx\n",
      "model.layers.4.mlp.gate_proj.qweight\n",
      "model.layers.4.mlp.gate_proj.qzeros\n",
      "model.layers.4.mlp.gate_proj.scales\n",
      "model.layers.4.mlp.up_proj.g_idx\n",
      "model.layers.4.mlp.up_proj.qweight\n",
      "model.layers.4.mlp.up_proj.qzeros\n",
      "model.layers.4.mlp.up_proj.scales\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.k_proj.g_idx\n",
      "model.layers.4.self_attn.k_proj.qweight\n",
      "model.layers.4.self_attn.k_proj.qzeros\n",
      "model.layers.4.self_attn.k_proj.scales\n",
      "model.layers.4.self_attn.o_proj.g_idx\n",
      "model.layers.4.self_attn.o_proj.qweight\n",
      "model.layers.4.self_attn.o_proj.qzeros\n",
      "model.layers.4.self_attn.o_proj.scales\n",
      "model.layers.4.self_attn.q_proj.g_idx\n",
      "model.layers.4.self_attn.q_proj.qweight\n",
      "model.layers.4.self_attn.q_proj.qzeros\n",
      "model.layers.4.self_attn.q_proj.scales\n",
      "model.layers.4.self_attn.v_proj.g_idx\n",
      "model.layers.4.self_attn.v_proj.qweight\n",
      "model.layers.4.self_attn.v_proj.qzeros\n",
      "model.layers.4.self_attn.v_proj.scales\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.mlp.down_proj.g_idx\n",
      "model.layers.5.mlp.down_proj.qweight\n",
      "model.layers.5.mlp.down_proj.qzeros\n",
      "model.layers.5.mlp.down_proj.scales\n",
      "model.layers.5.mlp.gate_proj.g_idx\n",
      "model.layers.5.mlp.gate_proj.qweight\n",
      "model.layers.5.mlp.gate_proj.qzeros\n",
      "model.layers.5.mlp.gate_proj.scales\n",
      "model.layers.5.mlp.up_proj.g_idx\n",
      "model.layers.5.mlp.up_proj.qweight\n",
      "model.layers.5.mlp.up_proj.qzeros\n",
      "model.layers.5.mlp.up_proj.scales\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.k_proj.g_idx\n",
      "model.layers.5.self_attn.k_proj.qweight\n",
      "model.layers.5.self_attn.k_proj.qzeros\n",
      "model.layers.5.self_attn.k_proj.scales\n",
      "model.layers.5.self_attn.o_proj.g_idx\n",
      "model.layers.5.self_attn.o_proj.qweight\n",
      "model.layers.5.self_attn.o_proj.qzeros\n",
      "model.layers.5.self_attn.o_proj.scales\n",
      "model.layers.5.self_attn.q_proj.g_idx\n",
      "model.layers.5.self_attn.q_proj.qweight\n",
      "model.layers.5.self_attn.q_proj.qzeros\n",
      "model.layers.5.self_attn.q_proj.scales\n",
      "model.layers.5.self_attn.v_proj.g_idx\n",
      "model.layers.5.self_attn.v_proj.qweight\n",
      "model.layers.5.self_attn.v_proj.qzeros\n",
      "model.layers.5.self_attn.v_proj.scales\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.mlp.down_proj.g_idx\n",
      "model.layers.6.mlp.down_proj.qweight\n",
      "model.layers.6.mlp.down_proj.qzeros\n",
      "model.layers.6.mlp.down_proj.scales\n",
      "model.layers.6.mlp.gate_proj.g_idx\n",
      "model.layers.6.mlp.gate_proj.qweight\n",
      "model.layers.6.mlp.gate_proj.qzeros\n",
      "model.layers.6.mlp.gate_proj.scales\n",
      "model.layers.6.mlp.up_proj.g_idx\n",
      "model.layers.6.mlp.up_proj.qweight\n",
      "model.layers.6.mlp.up_proj.qzeros\n",
      "model.layers.6.mlp.up_proj.scales\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.k_proj.g_idx\n",
      "model.layers.6.self_attn.k_proj.qweight\n",
      "model.layers.6.self_attn.k_proj.qzeros\n",
      "model.layers.6.self_attn.k_proj.scales\n",
      "model.layers.6.self_attn.o_proj.g_idx\n",
      "model.layers.6.self_attn.o_proj.qweight\n",
      "model.layers.6.self_attn.o_proj.qzeros\n",
      "model.layers.6.self_attn.o_proj.scales\n",
      "model.layers.6.self_attn.q_proj.g_idx\n",
      "model.layers.6.self_attn.q_proj.qweight\n",
      "model.layers.6.self_attn.q_proj.qzeros\n",
      "model.layers.6.self_attn.q_proj.scales\n",
      "model.layers.6.self_attn.v_proj.g_idx\n",
      "model.layers.6.self_attn.v_proj.qweight\n",
      "model.layers.6.self_attn.v_proj.qzeros\n",
      "model.layers.6.self_attn.v_proj.scales\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.mlp.down_proj.g_idx\n",
      "model.layers.7.mlp.down_proj.qweight\n",
      "model.layers.7.mlp.down_proj.qzeros\n",
      "model.layers.7.mlp.down_proj.scales\n",
      "model.layers.7.mlp.gate_proj.g_idx\n",
      "model.layers.7.mlp.gate_proj.qweight\n",
      "model.layers.7.mlp.gate_proj.qzeros\n",
      "model.layers.7.mlp.gate_proj.scales\n",
      "model.layers.7.mlp.up_proj.g_idx\n",
      "model.layers.7.mlp.up_proj.qweight\n",
      "model.layers.7.mlp.up_proj.qzeros\n",
      "model.layers.7.mlp.up_proj.scales\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.k_proj.g_idx\n",
      "model.layers.7.self_attn.k_proj.qweight\n",
      "model.layers.7.self_attn.k_proj.qzeros\n",
      "model.layers.7.self_attn.k_proj.scales\n",
      "model.layers.7.self_attn.o_proj.g_idx\n",
      "model.layers.7.self_attn.o_proj.qweight\n",
      "model.layers.7.self_attn.o_proj.qzeros\n",
      "model.layers.7.self_attn.o_proj.scales\n",
      "model.layers.7.self_attn.q_proj.g_idx\n",
      "model.layers.7.self_attn.q_proj.qweight\n",
      "model.layers.7.self_attn.q_proj.qzeros\n",
      "model.layers.7.self_attn.q_proj.scales\n",
      "model.layers.7.self_attn.v_proj.g_idx\n",
      "model.layers.7.self_attn.v_proj.qweight\n",
      "model.layers.7.self_attn.v_proj.qzeros\n",
      "model.layers.7.self_attn.v_proj.scales\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.mlp.down_proj.g_idx\n",
      "model.layers.8.mlp.down_proj.qweight\n",
      "model.layers.8.mlp.down_proj.qzeros\n",
      "model.layers.8.mlp.down_proj.scales\n",
      "model.layers.8.mlp.gate_proj.g_idx\n",
      "model.layers.8.mlp.gate_proj.qweight\n",
      "model.layers.8.mlp.gate_proj.qzeros\n",
      "model.layers.8.mlp.gate_proj.scales\n",
      "model.layers.8.mlp.up_proj.g_idx\n",
      "model.layers.8.mlp.up_proj.qweight\n",
      "model.layers.8.mlp.up_proj.qzeros\n",
      "model.layers.8.mlp.up_proj.scales\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.k_proj.g_idx\n",
      "model.layers.8.self_attn.k_proj.qweight\n",
      "model.layers.8.self_attn.k_proj.qzeros\n",
      "model.layers.8.self_attn.k_proj.scales\n",
      "model.layers.8.self_attn.o_proj.g_idx\n",
      "model.layers.8.self_attn.o_proj.qweight\n",
      "model.layers.8.self_attn.o_proj.qzeros\n",
      "model.layers.8.self_attn.o_proj.scales\n",
      "model.layers.8.self_attn.q_proj.g_idx\n",
      "model.layers.8.self_attn.q_proj.qweight\n",
      "model.layers.8.self_attn.q_proj.qzeros\n",
      "model.layers.8.self_attn.q_proj.scales\n",
      "model.layers.8.self_attn.v_proj.g_idx\n",
      "model.layers.8.self_attn.v_proj.qweight\n",
      "model.layers.8.self_attn.v_proj.qzeros\n",
      "model.layers.8.self_attn.v_proj.scales\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.mlp.down_proj.g_idx\n",
      "model.layers.9.mlp.down_proj.qweight\n",
      "model.layers.9.mlp.down_proj.qzeros\n",
      "model.layers.9.mlp.down_proj.scales\n",
      "model.layers.9.mlp.gate_proj.g_idx\n",
      "model.layers.9.mlp.gate_proj.qweight\n",
      "model.layers.9.mlp.gate_proj.qzeros\n",
      "model.layers.9.mlp.gate_proj.scales\n",
      "model.layers.9.mlp.up_proj.g_idx\n",
      "model.layers.9.mlp.up_proj.qweight\n",
      "model.layers.9.mlp.up_proj.qzeros\n",
      "model.layers.9.mlp.up_proj.scales\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.k_proj.g_idx\n",
      "model.layers.9.self_attn.k_proj.qweight\n",
      "model.layers.9.self_attn.k_proj.qzeros\n",
      "model.layers.9.self_attn.k_proj.scales\n",
      "model.layers.9.self_attn.o_proj.g_idx\n",
      "model.layers.9.self_attn.o_proj.qweight\n",
      "model.layers.9.self_attn.o_proj.qzeros\n",
      "model.layers.9.self_attn.o_proj.scales\n",
      "model.layers.9.self_attn.q_proj.g_idx\n",
      "model.layers.9.self_attn.q_proj.qweight\n",
      "model.layers.9.self_attn.q_proj.qzeros\n",
      "model.layers.9.self_attn.q_proj.scales\n",
      "model.layers.9.self_attn.v_proj.g_idx\n",
      "model.layers.9.self_attn.v_proj.qweight\n",
      "model.layers.9.self_attn.v_proj.qzeros\n",
      "model.layers.9.self_attn.v_proj.scales\n",
      "model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "for n in tensors.keys():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0039,  0.0032, -0.0071,  ...,  0.0053, -0.0082,  0.0070],\n",
       "        [-0.0315,  0.0466, -0.0023,  ..., -0.0211,  0.0173,  0.0334],\n",
       "        [-0.0125,  0.0036,  0.0195,  ..., -0.0271,  0.0143, -0.0082],\n",
       "        ...,\n",
       "        [-0.0281, -0.0195, -0.0024,  ...,  0.0123, -0.0117, -0.0237],\n",
       "        [ 0.0229,  0.0255,  0.0315,  ...,  0.0067, -0.0092, -0.0058],\n",
       "        [ 0.0080, -0.0088,  0.0063,  ..., -0.0293, -0.0200,  0.0337]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[\"lm_head.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
       "         -6.5565e-06,  8.9407e-07],\n",
       "        [ 1.8616e-03, -3.3722e-03,  3.9864e-04,  ..., -8.3008e-03,\n",
       "          2.5787e-03, -3.9368e-03],\n",
       "        [ 1.0986e-02,  9.8877e-03, -5.0964e-03,  ...,  2.5177e-03,\n",
       "          7.7057e-04, -5.0049e-03],\n",
       "        ...,\n",
       "        [-1.3977e-02, -2.7313e-03, -1.9897e-02,  ..., -1.0437e-02,\n",
       "          9.5825e-03, -1.8005e-03],\n",
       "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
       "         -1.6357e-02,  3.3875e-03],\n",
       "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
       "         -1.2939e-02,  3.1948e-05]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[\"model.embed_tokens.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1432637530,  1423070549,  1721063833,  ...,  -425284886,\n",
       "         -1430327635,  1872685802],\n",
       "        [ 1924038393,  1427526790, -1587128151,  ..., -1381307665,\n",
       "         -1783010379,  -357193046],\n",
       "        [-1079010710, -1583306411, -1773051580,  ...,  -356584870,\n",
       "         -2023237782, -1957041510],\n",
       "        ...,\n",
       "        [-1431673253, -1344094806, -1427514982,  ...,   354755717,\n",
       "           342168167, -2035651925],\n",
       "        [-1646876996,  1767896730,   988523238,  ...,  1499546965,\n",
       "         -1247455719, -1164535134],\n",
       "        [-1515587093,  -573216154,  -169428358,  ..., -1726638826,\n",
       "            73688614,  1788533353]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[\"model.layers.0.mlp.down_proj.qweight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = eval_ppl(model, False, \"llama2\", \"cuda\", base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"ppl_results/EX1/llama2-7b-omni_w4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped = json.dumps(\n",
    "    results, indent=2, ensure_ascii=False\n",
    ")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    with open(os.path.join(output_dir, \"results.json\"), \"w\") as f:\n",
    "        f.write(dumped)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniquant_kernel",
   "language": "python",
   "name": "omniquant"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
